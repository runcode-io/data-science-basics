{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c902c2d9-7b2f-401e-b09c-79885e86be11",
   "metadata": {},
   "source": [
    "# 6. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4bab4-2393-454e-83d8-75b24140e35b",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b79b9-8e7c-44a5-80dc-02718106a204",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/clustering-in-machine-learning2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68643d07-b993-4de9-a0ad-9ca362e43c56",
   "metadata": {},
   "source": [
    "<b>Clustering</b> is a machine learning technique that involves dividing a dataset into groups (also known as clusters) based on the patterns within the data. The goal of clustering is to split the data into groups such that the data within each group is more similar to each other than to data in other groups. This technique is useful for a variety of purposes, such as data compression, anomaly detection, and summarizing data.\n",
    "\n",
    "There are many different clustering algorithms available, and the choice of which one to use depends on the characteristics of the data and the goals of the analysis. Some common types of clustering algorithms include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bbc2ee-99b7-48df-ba4e-cc9fdf02c270",
   "metadata": {},
   "source": [
    "1. K-Means Clustering: This is one of the most popular clustering algorithms, and it works by iteratively dividing the data into K clusters based on the distance between data points and the centroid (mean) of each cluster.\n",
    "\n",
    "2. Hierarchical Clustering: This type of clustering algorithm builds a hierarchy of clusters, where each cluster is nested within another cluster. There are two main types of hierarchical clustering: Agglomerative and Divisive. Agglomerative clustering starts with each data point as its own cluster, and then merges the closest pairs of clusters until there is only one cluster left. Divisive clustering, on the other hand, starts with all data points in one cluster and then splits the cluster into smaller and smaller clusters.\n",
    "\n",
    "3. DBSCAN: This stands for Density-Based Spatial Clustering of Applications with Noise. It is a density-based clustering algorithm that works by identifying \"dense\" clusters of points in the data and marking points that are not part of a dense cluster as noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b4347b-af14-4aff-8f7d-645b998403f8",
   "metadata": {},
   "source": [
    "### 6.1 K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96752d4-2859-444b-9e41-3dc1d9378d03",
   "metadata": {},
   "source": [
    "<b>K-Means clustering</b> is used to divide a dataset into K clusters based on the similarity of the data points. The goal of the algorithm is to minimize the within-cluster sum of squares, which is the sum of the squared distances between the data points and the centroid (mean) of their cluster.\n",
    "\n",
    "The algorithm works by starting with a set of K randomly chosen initial centroids, and then iteratively assigning each data point to the closest centroid and updating the centroids to the mean of the points assigned to it. This process continues until the centroids stop changing or a predetermined number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec17383-8013-4594-b66f-c80b15655e05",
   "metadata": {},
   "source": [
    "Here is an example of K-Means clustering in Python:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b526bbc9-7a97-41e3-8ad9-75f5beb7809d",
   "metadata": {},
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load the data\n",
    "X = ...\n",
    "\n",
    "# Initialize the KMeans model with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict the cluster labels for each data point\n",
    "labels = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6a619-a32a-414d-9c96-c76737d1d5c2",
   "metadata": {},
   "source": [
    "### 6.2 Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f81f1d4-e78a-49c0-9ddb-51975d0e56a5",
   "metadata": {},
   "source": [
    "<b>Hierarchical clustering</b> is used to group data points into a hierarchy of clusters. There are two main types of hierarchical clustering: Agglomerative and Divisive.\n",
    "\n",
    "* Agglomerative Clustering: This type of hierarchical clustering starts with each data point as its own cluster, and then iteratively merges the closest pairs of clusters until there is only one cluster left. The main advantage of agglomerative clustering is that it is relatively simple to implement and understand.\n",
    "\n",
    "* Divisive Clustering: This type of hierarchical clustering starts with all data points in one cluster and then iteratively splits the cluster into smaller and smaller clusters until each data point is in its own cluster. Divisive clustering can be more computationally expensive than agglomerative clustering, but it can be useful in certain situations.\n",
    "\n",
    "Here is an example of agglomerative clustering in Python using the sklearn library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cca93708-c52f-46d3-94b4-7eaba4d98a01",
   "metadata": {},
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Load the data\n",
    "X = ...\n",
    "\n",
    "# Initialize the AgglomerativeClustering model with 3 clusters\n",
    "agglomerative_clustering = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "# Fit the model to the data\n",
    "agglomerative_clustering.fit(X)\n",
    "\n",
    "# Predict the cluster labels for each data point\n",
    "labels = agglomerative_clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4b4182-e44d-42f0-b8aa-4ef3e2ee97e2",
   "metadata": {},
   "source": [
    "### 6.3 DBSCAN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2452e92-2c9a-4132-9387-2c9093d67ac9",
   "metadata": {},
   "source": [
    "<b>DBSCAN</b> (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that is used to identify \"dense\" clusters of points in a dataset and mark points that are not part of a dense cluster as noise. It works by identifying points that have a high number of nearby points (which are considered part of the same cluster) and expanding the cluster to include all points that are reachable from those points, as long as they meet a minimum density threshold.\n",
    "\n",
    "One of the main advantages of DBSCAN is that it does not require the user to specify the number of clusters in advance. Instead, it automatically detects the number of clusters based on the density of the data.\n",
    "\n",
    "Here is an example of DBSCAN in Python using the sklearn library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ccf5c0d-76ff-423e-bb0b-d136a95103ff",
   "metadata": {},
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Load the data\n",
    "X = ...\n",
    "\n",
    "# Initialize the DBSCAN model\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "# Fit the model to the data\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Predict the cluster labels for each data point\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd56db0-6596-4100-8702-83c45ae8c267",
   "metadata": {},
   "source": [
    "### 6.4 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0ef337-2797-4cff-b3f1-f3f26c5117a1",
   "metadata": {},
   "source": [
    "<b>Dimensionality Reduction</b> is a technique used to reduce the number of features (dimensions) in a dataset while retaining as much of the information as possible. It is often used in conjunction with clustering algorithms to make the clustering process more efficient and to improve the interpretability of the results.\n",
    "\n",
    "There are many different dimensionality reduction techniques available, including Principal Component Analysis (PCA), Singular Value Decomposition (SVD), and t-SNE (t-Distributed Stochastic Neighbor Embedding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dec46b2-c4c2-4818-b60f-6fc98d55d63c",
   "metadata": {},
   "source": [
    "<b>Principal Component Analysis (PCA)</b>:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4971c4b-2ed9-433e-a0ca-e0522d9325f8",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a popular dimensionality reduction technique that is used to project high-dimensional data onto a lower-dimensional space while retaining as much of the original variance as possible. It does this by finding the directions in which the data varies the most and using these directions as the new axes of the lower-dimensional space.\n",
    "\n",
    "There are several techniques for performing PCA, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8905f44e-c99c-42cc-a712-e9ea4a9d5491",
   "metadata": {},
   "source": [
    "1. Standard PCA: This is the most common technique for performing PCA. It involves calculating the covariance matrix of the data, finding the eigenvectors and eigenvalues of the covariance matrix, and then selecting the top k eigenvectors (where k is the number of dimensions in the lower-dimensional space) to form the projection matrix.\n",
    "\n",
    "2. Kernel PCA: This technique is used when the data is not linearly separable and cannot be projected onto a lower-dimensional space using standard PCA. It involves using a kernel function to transform the data into a higher-dimensional space where it becomes linearly separable, and then applying standard PCA to this transformed data.\n",
    "\n",
    "3. Incremental PCA: This technique is used when the data is too large to fit in memory, and it allows you to perform PCA in a streaming fashion by processing the data in small batches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e3a53-5032-4e31-8874-942535db8c94",
   "metadata": {},
   "source": [
    "<b>Steps involved in PCA are:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761c66f-4ad6-437b-b782-f1b450351bf6",
   "metadata": {},
   "source": [
    "1. Standardize the data: PCA is sensitive to the scaling of the data, so it is important to standardize the data before applying PCA. This can be done by subtracting the mean from each feature and dividing by the standard deviation.\n",
    "\n",
    "2. Calculate the covariance matrix: The next step is to calculate the covariance matrix of the standardized data. The covariance matrix is a square matrix that gives the covariance between all pairs of features in the data.\n",
    "\n",
    "3. Calculate the eigenvectors and eigenvalues of the covariance matrix: The eigenvectors and eigenvalues of the covariance matrix are used to project the data onto the lower-dimensional space. The eigenvectors are the directions in which the data varies the most, and the eigenvalues are the magnitudes of the variations along these directions.\n",
    "\n",
    "4. Sort the eigenvectors by the eigenvalues: The eigenvectors should be sorted in decreasing order of the eigenvalues, as the eigenvectors corresponding to the largest eigenvalues are the ones that capture the most variance in the data.\n",
    "\n",
    "5. Select the top k eigenvectors: The top k eigenvectors (where k is the number of dimensions in the lower-dimensional space) are used to form the projection matrix.\n",
    "\n",
    "6. Project the data onto the lower-dimensional space: The projection matrix is used to transform the data onto the lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f32c4-840d-48f6-a139-78540e1eeb9f",
   "metadata": {},
   "source": [
    "Here's a simple example of how standard PCA can be implemented in Python using the popular scikit-learn library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42306c96-dac5-42d7-9779-b0e039c99d43",
   "metadata": {},
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the data\n",
    "X = ...\n",
    "\n",
    "# Initialize the PCA model with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the model to the data and transform it\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14abe2a5-3582-4ad1-9c7f-e451c4a1fece",
   "metadata": {},
   "source": [
    "In the above example, X is the high-dimensional data and X_transformed is the transformed data in the lower-dimensional space. The fit method calculates the eigenvectors and eigenvalues of the data and the transform method projects the data onto the lower-dimensional space using these eigenvectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
