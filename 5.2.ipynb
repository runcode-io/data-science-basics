{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7f3a14-d9da-4902-a256-bbf7a6f3ed97",
   "metadata": {},
   "source": [
    "# 5.2 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2283e2dc-0a44-47fe-aa51-429252d1cbcc",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/classification-algorithm-in-machine-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f24fd18-466a-4836-bdc0-b6fbae50d3ce",
   "metadata": {},
   "source": [
    "Classification Algorithms can be divided into the mainly two category:\n",
    "\n",
    "* Linear Models\n",
    "    * Logistic Regression\n",
    "    * Support Vector Machines(SVM)\n",
    "* Non-linear Models\n",
    "    * K-Nearest Neighbours\n",
    "    * Na√Øve Bayes\n",
    "    * Decision Tree Classification\n",
    "    * Random Forest Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92aa85b-48c1-48bb-b466-c01e712f5128",
   "metadata": {},
   "source": [
    "## 5.2.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430d3dc-2653-4abd-abcf-32a3931d48a0",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/logistic-regression-in-machine-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda4f602-3512-463d-83bd-e64f813731f7",
   "metadata": {},
   "source": [
    "<b>Logistic regression</b> is a classification algorithm used for predicting the probability of a binary outcome. It is a linear method that models the relationship between the dependent variable (output) and one or more independent variables (inputs) using a logistic function.\n",
    "\n",
    "The logistic function, also known as the sigmoid function, is defined as:\n",
    "\n",
    "f(x) = 1 / (1 + e^-x)\n",
    "\n",
    "It maps the input x to a value between 0 and 1, which can be interpreted as the probability of a positive outcome (e.g. 1 for a binary classification problem).\n",
    "\n",
    "Here is an example of logistic regression in Python using scikit-learn:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35e83978-9356-4cbb-91b0-cc6c4fa5496b",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ae828-9b2d-467f-8d18-e766c9f22c85",
   "metadata": {},
   "source": [
    "In this example, we split the data into training and test sets, initialize a logistic regression model, fit the model to the training data, make predictions on the test data, and evaluate the model performance using the accuracy score.\n",
    "\n",
    "Logistic regression is a simple and widely used method for classification, and it is effective for many applications. However, it is limited to binary classification and is sensitive to the assumption of linearity between the independent variables and the logit of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59b30c-4622-4bc7-9dbf-c912dc5d32ca",
   "metadata": {},
   "source": [
    "## 5.2.2 Support Vector Machines(SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4affeb69-59a3-400b-9305-a69bd377231f",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15618413-bb16-4e37-b6cf-b370105dabea",
   "metadata": {},
   "source": [
    "A <b>support vector machine (SVM)</b> is a supervised learning algorithm that can be used for classification or regression. In the case of classification, the algorithm creates a hyperplane or set of hyperplanes in a high-dimensional space, which can be used to classify new data points.\n",
    "\n",
    "An SVM is particularly well-suited for classification of complex but small- or medium-sized datasets.\n",
    "\n",
    "Here is a simple example of how to train and use an SVM in Python using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261f1eca-48d6-49f8-afd7-93fa418c151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "# Load the iris dataset as an example\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "Y = iris[\"target\"]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Create an SVM model\n",
    "model = svm.SVC()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Test the model on the test data\n",
    "accuracy = model.score(X_test, Y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94083da1-72ec-43b4-920b-24f2f39b3a7e",
   "metadata": {},
   "source": [
    "This code will train an SVM on the iris dataset and print the accuracy of the model on the test set. The iris dataset is a classic dataset in machine learning, which consists of measurements of various iris flowers and the species of iris that the measurements correspond to. The SVM will learn from the training data to predict the species of a new iris flower based on its measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151fbfb-a8b0-4f1c-99d5-71564e3a4f39",
   "metadata": {},
   "source": [
    "## 5.2.3 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41deae6-ba2e-4acd-9966-6e86d8f4353a",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/decision-tree-classification-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1786f3a-767d-4b46-a2f3-a5d943391b60",
   "metadata": {},
   "source": [
    "<b>Decision tree</b> classification is a supervised learning algorithm that can be used for classification tasks. It works by creating a tree-like model of decisions based on the features of the data.\n",
    "\n",
    "At each internal node of the tree, the algorithm selects the feature that maximizes the information gain at that node, and then creates branches based on the possible values of that feature. This process is repeated recursively on each branch until the leaves of the tree are reached, at which point the predicted class is determined.\n",
    "\n",
    "Here is an example of how to train and use a decision tree classifier in Python using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103c9d4f-05f5-4d3a-80ae-256c67a5e402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "# Load the iris dataset as an example\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "Y = iris[\"target\"]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Create a decision tree classifier\n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Test the model on the test data\n",
    "accuracy = model.score(X_test, Y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60356d63-f259-498d-ba19-69353b16ebdd",
   "metadata": {},
   "source": [
    "This code will train a decision tree classifier on the iris dataset and print the accuracy of the model on the test set. The iris dataset is a classic dataset in machine learning, which consists of measurements of various iris flowers and the species of iris that the measurements correspond to. The decision tree classifier will learn from the training data to predict the species of a new iris flower based on its measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46b95e-f1fc-4505-a2c5-ee6251c12edb",
   "metadata": {},
   "source": [
    "## 5.2.4 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bbb010-db58-493a-be09-d20eb36b3081",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973d6315-fe05-4bb9-a8ee-b34f1f8f7465",
   "metadata": {},
   "source": [
    "<b>Random forest</b> classification is an ensemble learning method that is used for classification tasks. It works by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "One of the key features of random forest classifiers is that they can handle a large number of features, and they can still make accurate predictions even if some of the features are correlated or if there are nonlinear relationships between the features and the target.\n",
    "\n",
    "Here is an example of how to train and use a random forest classifier in Python using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "955fc531-4328-40aa-8412-d945d42f00b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load the iris dataset as an example\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "Y = iris[\"target\"]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Create a random forest classifier\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Test the model on the test data\n",
    "accuracy = model.score(X_test, Y_test)\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c2f28a-6fb8-47f9-b597-7d47a0da1c4c",
   "metadata": {},
   "source": [
    "### Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956a2a3-c3fc-4712-a3ee-ea6b5dddc9bc",
   "metadata": {},
   "source": [
    "Bagging and boosting are ensemble learning methods that can be used to improve the performance of a machine learning model.\n",
    "\n",
    "<b>Bagging</b> (short for bootstrapped aggregation) is a method that involves training multiple models on different subsets of the training data and then combining their predictions. This can be done using decision trees, neural networks, or any other type of model. The idea behind bagging is that the combination of the predictions from the multiple models will be more accurate than the predictions of any individual model.\n",
    "\n",
    "<b>Boosting</b> is a method that involves training multiple models sequentially, with each model attempting to correct the mistakes of the previous model. Boosting algorithms typically use decision trees, but they can also be used with other types of models. The main idea behind boosting is to train a weak model, and then to iteratively improve it by adding new models that focus on the mistakes made by the previous models. Boosting algorithms can often achieve higher accuracy than bagging, but they are also more prone to overfitting.\n",
    "\n",
    "Here is an example of how to use bagging and boosting in Python using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9e48f7-8639-4aaf-af1a-b5aa69e4a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging accuracy: 0.9666666666666667\n",
      "Boosting accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Load the iris dataset as an example\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"]\n",
    "Y = iris[\"target\"]\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Create a bagging classifier\n",
    "bagging_model = BaggingClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "bagging_model.fit(X_train, Y_train)\n",
    "\n",
    "# Test the model on the test data\n",
    "bagging_accuracy = bagging_model.score(X_test, Y_test)\n",
    "\n",
    "# Create an AdaBoost classifier\n",
    "boosting_model = AdaBoostClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "boosting_model.fit(X_train, Y_train)\n",
    "\n",
    "# Test the model on the test data\n",
    "boosting_accuracy = boosting_model.score(X_test, Y_test)\n",
    "\n",
    "# Print the accuracies\n",
    "print(\"Bagging accuracy:\", bagging_accuracy)\n",
    "print(\"Boosting accuracy:\", boosting_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a93a1a-f9f3-438c-8c7e-9b0d25814eab",
   "metadata": {},
   "source": [
    "## 5.2.5 K-nearest neighbors (KNN) classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7fa07-e0ca-47e6-8460-acad517bf021",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ca224-2708-4961-ac1e-2cab85569d9b",
   "metadata": {},
   "source": [
    "The <b>K-nearest neighbors (KNN) classifier</b> is a simple, easy-to-implement supervised machine learning algorithm that can be used to solve both classification and regression problems. It's a lazy learning algorithm because it doesn't have a specialized training phase. Instead, it uses all of the data for training while classifying a new data point or instance.\n",
    "\n",
    "Here's a simple example of how a KNN classifier can be implemented in Python using the popular scikit-learn library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "09b1d693-ed0d-4a7a-bcc2-43f8f057bb9c",
   "metadata": {},
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Creating a KNN classifier with k=3 (default value)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Training the classifier with training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the labels for the test data\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa016a0-ab2f-4523-bcd1-5dc3bc357a37",
   "metadata": {},
   "source": [
    "The parameter 'n_neighbors' specifies the number of nearest neighbors that the classifier should consider while predicting the label of a new data point. By default, it is set to 5. You can specify any other value for n_neighbors depending on the size and nature of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d388a804-708e-4efe-8bf1-8b33f4593a98",
   "metadata": {},
   "source": [
    "## 5.2.6 XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05994248-5c75-4667-885d-ce3fd8ff96f4",
   "metadata": {},
   "source": [
    "<b>XGBoost</b> (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosting framework for machine learning. It has a number of hyperparameters that can be tuned to achieve better performance, and it is widely used in a variety of machine learning tasks such as classification, regression, and ranking.\n",
    "\n",
    "Here's a simple example of how XGBoost can be implemented in Python using the popular scikit-learn library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "720823e6-88b3-4763-a9f7-79b888c2c781",
   "metadata": {},
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Creating an XGBoost classifier\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "# Training the classifier with training data\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the labels for the test data\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "# Calculating the accuracy of the model\n",
    "accuracy = xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8dbe04-ddc2-40e9-9f8d-108ead7993f1",
   "metadata": {},
   "source": [
    "There are several hyperparameters that you can tune to improve the performance of the XGBoost model. Some of the important ones are:\n",
    "\n",
    "* max_depth: maximum depth of the tree.\n",
    "* learning_rate: learning rate for the boosting.\n",
    "* n_estimators: number of trees in the ensemble.\n",
    "* gamma: minimum loss reduction required to make a split.\n",
    "\n",
    "You can specify these hyperparameters while creating the XGBoost model like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d7553b-ee1a-4682-ae77-6be125423a69",
   "metadata": {},
   "source": [
    "xgb = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, gamma=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
