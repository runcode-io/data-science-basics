{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b2c790-9fc5-4618-9d1b-4046ee6ab8e3",
   "metadata": {},
   "source": [
    "# Model selection and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a0a70-ef54-416a-af16-deb17c9651e2",
   "metadata": {},
   "source": [
    "<b>Model selection and validation</b> are important tasks in the machine learning workflow that help you choose the best model for your data and ensure that the model is generalizable to new data.\n",
    "\n",
    "Here are the various steps and concepts involved in model selection and validation:\n",
    "\n",
    "1. Data preprocessing: This step involves cleaning and preparing the data for modeling. This includes tasks such as missing value imputation, feature scaling, and feature selection.\n",
    "\n",
    "2. Model selection: This step involves choosing the most suitable model for your data based on the problem type (classification, regression, etc.) and the characteristics of the data. It may involve comparing the performance of different models on the same data, using techniques such as train-test split or cross-validation.\n",
    "\n",
    "3. Hyperparameter tuning: Most machine learning models have a number of hyperparameters that need to be tuned to achieve optimal performance. This step involves finding the best values for these hyperparameters, usually through a process called grid search or random search.\n",
    "\n",
    "4. Model evaluation: Once a model has been selected and its hyperparameters have been tuned, it is important to evaluate its performance on new data to ensure that it is generalizable. This can be done using techniques such as train-test split or cross-validation.\n",
    "\n",
    "5. Model interpretation: This step involves interpreting the results of the model and understanding how it is making predictions. This can be done using techniques such as feature importance, partial dependence plots, and SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4ab71-f954-479d-8a0b-b924928e9228",
   "metadata": {},
   "source": [
    "There are several techniques for model selection and validation, including:\n",
    "\n",
    "* Train-test split: This is a simple technique where the data is randomly split into a training set and a test set, and the model is trained on the training set and evaluated on the test set. This helps to evaluate the model's performance on new data.\n",
    "\n",
    "* K-fold cross-validation: This technique involves dividing the data into k folds, training the model on k-1 folds, and evaluating it on the remaining fold. This process is repeated k times, with a different fold being used as the test set each time. The final evaluation score is the average of the scores obtained on each fold.\n",
    "\n",
    "* Stratified K-fold cross-validation: This technique is similar to K-fold cross-validation, but it ensures that the proportions of different classes in the folds are preserved. This is especially important when the classes are imbalanced.\n",
    "\n",
    "* Grid Search: This is a technique for hyperparameter tuning. It involves specifying a grid of hyperparameter values, training a model for each combination of hyperparameters, and evaluating the performance of each model. The hyperparameters that give the best performance are then chosen.\n",
    "\n",
    "* Random Search: This is another technique for hyperparameter tuning. It involves sampling random combinations of hyperparameters, training a model for each combination, and evaluating the performance of each model. The hyperparameters that give the best performance are then chosen.\n",
    "\n",
    "* Ensemble Methods: These are techniques that combine the predictions of multiple models to make more accurate predictions. Some examples of ensemble methods include bagging, boosting, and bootstrapped ensembles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824de54-0376-48a5-a87a-5567acea5a12",
   "metadata": {},
   "source": [
    "### 7.1 Train-test split\n",
    "\n",
    "Train-test split is a simple technique for evaluating the performance of a machine learning model on new data. It involves randomly dividing the data into a training set and a test set, training the model on the training set, and evaluating it on the test set.\n",
    "\n",
    "Here's a simple example of how train-test split can be implemented in Python using the popular scikit-learn library:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d65cf6e8-6967-43ad-bc02-5c0cc7ec0ff3",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Splitting the data into a training set (75%) and a test set (25%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Training the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model on the test data\n",
    "score = model.score(X_test, y_test)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1fc56-0340-4e96-8eb4-d23d5d5898bd",
   "metadata": {},
   "source": [
    "In the above example, X and y are the features and labels of the data, respectively. The train_test_split function is used to split the data into a training set and a test set, with the test_size parameter specifying the size of the test set (in this case, 25% of the data). The random_state parameter specifies the random seed to use when shuffling the data. The model is then trained on the training data using the fit method, and evaluated on the test data using the score method.\n",
    "\n",
    "Train-test split is a simple and effective technique for evaluating the performance of a model, but it has a limitation in that the model is only evaluated on a single test set. To get a more robust estimate of the model's performance, you can use techniques such as K-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc633ff-312c-4d8c-a9ec-892b53064271",
   "metadata": {},
   "source": [
    "### 7.2 Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65d1947-303c-4cc5-8526-99d6ba670019",
   "metadata": {},
   "source": [
    "<b>Cross-validation</b> is a technique used in machine learning to evaluate the performance of a model and to choose the best model for a given problem. It is a resampling method that involves dividing the data into a number of folds, training the model on a subset of the data, and evaluating it on the remaining folds.\n",
    "\n",
    "* K-fold cross-validation: In k-fold cross-validation, the data is divided into k folds, and the model is trained and evaluated k times, each time using a different fold as the evaluation set and the remaining folds as the training set. The final evaluation score is the average of the k scores.\n",
    "\n",
    "Here is an example of k-fold cross-validation in scikit-learn for a simple linear regression model:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab569134-1ed1-4148-8033-25eb16f689e8",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Creating a KFold object with k=5 (number of folds)\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "# Creating a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Evaluating the model using K-fold cross-validation\n",
    "scores = []\n",
    "for train_index, test_index in kfold.split(X):\n",
    "  X_train, X_test = X[train_index], X[test_index]\n",
    "  y_train, y_test = y[train_index], y[test_index]\n",
    "  model.fit(X_train, y_train)\n",
    "  score = model.score(X_test, y_test)\n",
    "  scores.append(score)\n",
    "\n",
    "# Calculating the mean and standard deviation of the scores\n",
    "mean_score = np.mean(scores)\n",
    "std_dev = np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b68ea-9f90-4e97-a647-e66736731e3a",
   "metadata": {},
   "source": [
    "In the above example, X and y are the features and labels of the data, respectively. The split method of the KFold object returns the indices of the training and test sets for each fold. The fit method is used to train the model on the training data, and the score method is used to evaluate the model on the test data. The mean and standard deviation of the scores are then calculated to get an idea of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d0509f-466c-4f4b-8638-862770b00cb4",
   "metadata": {},
   "source": [
    "### 7.3 Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdde636-1365-4346-bce6-443dc9497fda",
   "metadata": {},
   "source": [
    "<b>Regression metrics</b> are used to evaluate the performance of a regression model, and they are used to compare different models and to choose the best model for a given problem. Some common regression metrics are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848884cf-6c8a-48af-a5e8-2a272142c76d",
   "metadata": {},
   "source": [
    "* Mean Squared Error (MSE): The MSE is the average of the squared errors between the predicted values and the true values. It is defined as:\n",
    "\n",
    "    MSE = ($1/n$) ∑($y_i$ - $ŷ_i$)$^2$\n",
    "\n",
    "    where $y_i$ is the true value, $ŷ_i$ is the predicted value, and n is the number of examples. The MSE is sensitive to outliers, and it is commonly used for continuous target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286dc93f-85f9-4701-bfbe-a5d4ab3e7b46",
   "metadata": {},
   "source": [
    "* Root Mean Squared Error (RMSE): The RMSE is the square root of the MSE, and it is used to compare the magnitude of the error to the scale of the target variable. It is defined as:\n",
    "\n",
    "    RMSE = $√(MSE)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955fa56c-72dc-4de8-a257-64e930be7e43",
   "metadata": {},
   "source": [
    "* Mean Absolute Error (MAE): The MAE is the average of the absolute errors between the predicted values and the true values. It is defined as:\n",
    "\n",
    "    MAE = ($1/n$) ∑|$y_i$ - $ŷ_i$|\n",
    "\n",
    "    The MAE is less sensitive to outliers than the MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee15330-7784-4626-a207-dafb6bcaa570",
   "metadata": {},
   "source": [
    "* R-squared (R²): This is a measure of the goodness of fit of a model. It is defined as the proportion of the variance in the true values that is explained by the model. R² ranges from 0 to 1, with higher values indicating a better fit.\n",
    "\n",
    "* Adjusted R-squared (R²): This is a modified version of R² that adjusts for the number of variables in the model. It is defined as 1 - (1-R²)*(n-1)/(n-p-1), where n is the number of samples and p is the number of variables in the model. Adjusted R² is often used to compare models with different numbers of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afc19c9-7b1a-40a0-8515-358292d62e7d",
   "metadata": {},
   "source": [
    "### 7.4 Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf7582-8425-4686-b88d-68f89ea90038",
   "metadata": {},
   "source": [
    "In classification tasks, we often want to evaluate the performance of a classifier by comparing the predicted class labels to the true class labels. There are a number of metrics that can be used to evaluate the performance of a classifier, and the appropriate metric depends on the specific characteristics of the data and the goals of the analysis.\n",
    "\n",
    "Here are some common classification metrics and their terminologies:\n",
    "\n",
    "1. Accuracy: This is the most intuitive metric, and it is simply the fraction of correct predictions made by the classifier. It is defined as the number of correct predictions divided by the total number of predictions.\n",
    "\n",
    "2. Precision: This metric is often used along with recall, and it is a measure of the classifier's exactness. Precision is defined as the number of true positives divided by the sum of the true positives and false positives.\n",
    "\n",
    "3. Recall: This metric is also known as sensitivity or the true positive rate. It is a measure of the classifier's completeness, and it is defined as the number of true positives divided by the sum of the true positives and false negatives.\n",
    "\n",
    "4. F1 Score: This is a weighted average of precision and recall, and it is defined as the harmonic mean of precision and recall. The F1 score is useful when you want to balance precision and recall.\n",
    "\n",
    "5. AUC-ROC (Area Under the Receiver Operating Characteristic curve): This is a measure of the classifier's ability to distinguish between positive and negative classes. It is defined as the area under the curve of the receiver operating characteristic (ROC) curve, which is a plot of true positive rate against false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25d67a-60b5-407a-8990-d0943f1d92a7",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://www.researchgate.net/publication/328148379/figure/fig1/AS:679514740895744@1539020347601/Model-performance-metrics-Visual-representation-of-the-classification-model-metrics.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
