{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a2e5d54-2ad7-4377-a3d6-80164b78e880",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f15ac-263b-4b0f-a0b2-720fc52171c3",
   "metadata": {},
   "source": [
    "<b>Deep learning</b> is a subfield of machine learning that is inspired by the structure and function of the brain, specifically the neural networks that make up the brain. It involves training artificial neural networks (ANNs) on a large dataset, allowing the network to learn and make intelligent decisions on its own.\n",
    "\n",
    "Deep learning techniques are used in a variety of applications, including image and speech recognition, natural language processing, and machine translation.\n",
    "\n",
    "<b>Neural network</b> : \n",
    "A neural network is a type of machine learning model that is inspired by the structure and function of the brain. It is composed of layers of interconnected \"neurons,\" which process and transmit information.\n",
    "\n",
    "The basic structure of a neural network consists of an input layer, one or more hidden layers, and an output layer. The input layer receives the input data and passes it through the hidden layers, which use weights and biases to transform the data and extract features. The output layer produces the final output of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa7aa9b-9b83-464e-8ccd-285eba3b9084",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://www.researchgate.net/publication/330120030/figure/fig1/AS:735637925797888@1552401157053/Deep-Neural-Network-architecture.ppm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d8e478-a818-4e76-b007-6eae4d0f0ba8",
   "metadata": {},
   "source": [
    "Some key concepts and terminologies in deep learning include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7440e1f-b432-45f3-a95f-0bcc4bb47f3b",
   "metadata": {},
   "source": [
    "1. <b>Artificial neural networks:</b> These are networks of artificial neurons that are designed to process data in a way that is similar to how the brain processes information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af47771-bc0a-4ea6-a189-834903101f0d",
   "metadata": {},
   "source": [
    "2. <b>Perceptron</b>: Perceptron is a type of artificial neural network that is used for binary classification tasks. It is based on a single layer of artificial neurons, also known as perceptrons. Each perceptron receives input from data points in the dataset and uses a weighted sum of these inputs to make a prediction. If the weighted sum is above a certain threshold, the perceptron will classify the input as belonging to one class, otherwise it will classify it as belonging to the other class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a9bd8-e271-4f23-b961-062589220474",
   "metadata": {},
   "source": [
    "3. <b>Multi-layer perceptron (MLP)</b>: Multi-layer perceptron (MLP) is a type of artificial neural network that is composed of multiple layers of perceptrons. It is a feedforward neural network, which means that the information flows through the network in only one direction, from the input layer to the output layer, without looping back. MLPs can be used for a wide range of tasks, including classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556a86e-cfa3-4aae-aa02-33ff4fedea75",
   "metadata": {},
   "source": [
    "4. <b>Layers:</b> An artificial neural network is composed of multiple layers of interconnected neurons. Each layer processes the input data and passes it on to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970abb6a-3e88-40b1-bd4f-0b39bdcffa05",
   "metadata": {},
   "source": [
    "5. <b>Weights and biases</b>: Weights and biases are parameters of artificial neural networks that are learned during the training process. They are used to transform the input data and make predictions.\n",
    "\n",
    "    Weights are the values that are multiplied with the input data to produce the output of a neuron. They are represented as a matrix of values, with one row for each input and one column for each neuron in the next layer. For example, if a neural network has 3 inputs and 4 neurons in the next layer, the weights matrix would have a shape of (3, 4).\n",
    "\n",
    "    Biases are scalar values that are added to the output of a neuron after the weights have been applied. They are used to shift the activation function of the neuron and can help the network to make better predictions.\n",
    "\n",
    "    In deep learning, weights and biases are learned through the process of training the network. During training, the network is fed a batch of input data and the corresponding target output. The output of the network is compared to the target output, and the error is used to update the weights and biases of the neurons in the network using an optimization algorithm such as stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3b6c2-5f9c-455b-8231-93d5a7e6e728",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://miro.medium.com/max/1400/1*upfpVueoUuKPkyX3PR3KBg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b99e30-e138-4621-9c23-f1d04be37bf1",
   "metadata": {},
   "source": [
    "6. <b>Activation functions</b>: Activation functions are an important component of artificial neural networks. They determine the output of a neuron given its input and introduce nonlinearity into the network. Without activation functions, neural networks would be limited to linear models and would be unable to learn complex relationships in the data.\n",
    "\n",
    "    There are several types of activation functions that are commonly used in deep learning, including:\n",
    "\n",
    "    * Sigmoid: The sigmoid activation function maps any real-valued number to the range of 0 to 1. It is often used in the output layer of a binary classification model.\n",
    "    \n",
    "    * Tanh: The tanh activation function is similar to the sigmoid function but maps values to the range of -1 to 1. It is often used in the hidden layers of a neural network.\n",
    "    \n",
    "    * ReLU (Rectified Linear Unit): The ReLU activation function maps all negative values to 0 and all positive values to the same value. It is the most commonly used activation function in deep learning and is known for its simplicity and effectiveness.\n",
    "    \n",
    "    * Leaky ReLU: The leaky ReLU activation function is similar to the ReLU function but allows a small gradient when the input is negative. This helps to alleviate the \"dying ReLU\" problem, where the weights of a neuron become stuck at 0 and the neuron is unable to learn.\n",
    "    \n",
    "    * Softmax: The softmax function is a type of activation function that is often used in the output layer of a neural network for multi-class classification. It maps the output of the network to a probability distribution over the different classes, so that the predicted class is the one with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9e7892-a0cf-4b60-bdbf-f4be40d32f33",
   "metadata": {},
   "source": [
    "7. <b>Forward propagation</b>: Forward propagation is the process of passing the input data through an artificial neural network in order to generate an output. It is an essential step in the training and inference process of a neural network.\n",
    "\n",
    "    In forward propagation, the input data is passed through the input layer of the network and transformed by the weights and biases of the neurons in each successive layer. The output of each layer is then passed as input to the next layer until the output of the final layer is produced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f547efc-a49a-4358-bf26-57cb0600d4e3",
   "metadata": {},
   "source": [
    "8. <b>Loss function</b>: A loss function is a function that measures the error between the predicted output of a neural network and the true output. It is used to optimize the performance of the network by adjusting the weights and biases of the neurons.\n",
    "\n",
    "    There are many types of loss functions that are used in deep learning, depending on the task at hand. Some common loss functions include:\n",
    "\n",
    "    * Mean squared error (MSE): This loss function measures the average squared difference between the predicted and true output. It is often used for regression tasks.\n",
    "    \n",
    "    * Cross-entropy loss: This loss function is often used for classification tasks. It measures the difference between the predicted probability distribution and the true distribution.\n",
    "    \n",
    "    * Binary cross-entropy loss: This loss function is used for binary classification tasks. It measures the difference between the predicted probability and the true label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae525d5-9d00-4be2-84c8-b1207703ee7a",
   "metadata": {},
   "source": [
    "9. <b>Backpropagation</b>: Backpropagation is the process of adjusting the weights and biases of the neurons in a neural network based on the error between the predicted output and the true output. It is an important step in the training process of a neural network.\n",
    "\n",
    "    The process of backpropagation involves:\n",
    "\n",
    "    * Feeding the input data through the network to generate a predicted output.\n",
    "\n",
    "    * Calculating the error between the predicted output and the true output using the loss function.\n",
    "\n",
    "    * Propagating the error backwards through the network to update the weights and biases of the neurons.\n",
    "\n",
    "    * Repeating the process for multiple epochs until the network has learned to make predictions that are accurate enough for the given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91965d6f-05dc-4ba1-a807-29e2f4aeebf4",
   "metadata": {},
   "source": [
    "10. <b>Optimization</b>: Optimization is the process of finding the optimal values for the parameters of a model to minimize the error between the predicted output and the true output. In deep learning, optimization algorithms are used to adjust the weights and biases of the neurons in a neural network to minimize the loss function.\n",
    "\n",
    "    There are many optimization algorithms that are used in deep learning, each with its own strengths and weaknesses. Some common optimization algorithms include:\n",
    "\n",
    "    * Stochastic gradient descent (SGD): This is a simple optimization algorithm that involves iteratively updating the weights and biases of the neurons in the direction of the negative gradient of the loss function.\n",
    "    \n",
    "    * Momentum: This optimization algorithm is an extension of SGD that uses the past gradients to smooth out the update and reduce oscillation.\n",
    "    \n",
    "    * Adagrad: This optimization algorithm adjusts the learning rate of each weight based on its past gradients, so that frequently updated weights have a lower learning rate.\n",
    "    \n",
    "    * Adam: This optimization algorithm combines the ideas of momentum and Adagrad to provide fast and stable convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f26a35-b6e7-4ede-a824-f7c6ae2bc674",
   "metadata": {},
   "source": [
    "### Hyperparameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0019242-1c22-44af-97fd-29223345a7a9",
   "metadata": {},
   "source": [
    "Hyperparameters are the parameters of a neural network that are set before training. They control the overall behavior of the network and are an important factor in determining the performance of the model. Some common hyperparameters in deep learning include:\n",
    "\n",
    "1. Learning rate: The learning rate is a hyperparameter that controls the step size at which the optimizer makes updates to the weights and biases of the neurons. A smaller learning rate may lead to slower convergence, but may also result in a better solution.\n",
    "\n",
    "2. Batch size: The batch size is the number of samples that are processed by the network before the weights and biases are updated. A larger batch size may result in faster convergence, but may also require more memory.\n",
    "\n",
    "3. Number of epochs: The number of epochs is the number of times the entire dataset is passed through the network during training. A larger number of epochs may result in better performance, but may also lead to overfitting.\n",
    "\n",
    "4. Number of hidden units: The number of hidden units is the number of neurons in the hidden layers of the network. A larger number of hidden units may result in better performance, but may also increase the complexity of the model.\n",
    "\n",
    "5. Activation function: The activation function is the function that is applied to the output of each neuron to introduce nonlinearity into the network. Different activation functions can have a significant impact on the performance of the model.\n",
    "\n",
    "6. Weight initialization: The weight initialization is the method used to set the initial values of the weights of the neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad423cc-9db8-4a8b-9e22-2fdf88a919d7",
   "metadata": {},
   "source": [
    "Some of the most popular deep learning libraries include:\n",
    "\n",
    "1. <b>TensorFlow:</b> Developed by Google, TensorFlow is a powerful open-source library for training and deploying deep learning models. It has a large community of users and is highly customizable.\n",
    "\n",
    "2. <b>PyTorch:</b> PyTorch is an open-source library for training and deploying deep learning models. It is known for its flexibility and ease of use, and is popular for research and development of new ideas.\n",
    "\n",
    "3. <b>Keras:</b> Keras is a high-level deep learning library that runs on top of TensorFlow, PyTorch, or Theano. It is easy to use and allows users to quickly build and train deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84f339e-8655-4e4a-8ab4-880b78b1372a",
   "metadata": {},
   "source": [
    "There are several types of deep learning techniques, including:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa98f68-1b1d-4664-a572-1f3ccbaccd79",
   "metadata": {},
   "source": [
    "1. <b>Feedforward Neural Networks</b>: These are the most basic type of neural network, and they consist of a linear stack of layers where the output of one layer is fed as input to the next layer. They are used for tasks such as classification and regression.\n",
    "\n",
    "2. <b>Convolutional Neural Networks (CNNs)</b>: These are commonly used for image and video analysis tasks. They work by applying a series of filters to the input data to extract features and learn patterns in the data.\n",
    "\n",
    "3. <b>Recurrent Neural Networks (RNNs)</b>: These are commonly used for tasks that involve sequential data, such as language translation and speech recognition. They have the ability to retain memory of previous input and use that information to process current input.\n",
    "\n",
    "4. <b>Generative Adversarial Networks (GANs)</b>: These are used to generate new data that is similar to a given dataset. They consist of two networks: a generator network that produces new data and a discriminator network that tries to distinguish the generated data from the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c520e9-7987-40dd-838a-c52947d03186",
   "metadata": {},
   "source": [
    "Let us explore each and every deep learning techniques in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b7db2a-3dec-4a78-85cc-f4d19a687f7c",
   "metadata": {},
   "source": [
    "## 8.1 Feedforward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53f04d-e6ce-4247-ab57-c22880287e31",
   "metadata": {},
   "source": [
    "Feedforward neural networks, also known as fully-connected networks, are a type of artificial neural network where the neurons are fully connected and the data flows through the network in a single direction from the input layer to the output layer.\n",
    "\n",
    "Here is an example of how to implement a 1 Hidden Layer Feedforward Neural Network (ReLU Activation) in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fe734-06e8-479a-befd-c3f614cdaf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.22577865421772003. Accuracy: 91.38999938964844\n",
      "Iteration: 1000. Loss: 0.08613432943820953. Accuracy: 93.05000305175781\n",
      "Iteration: 1500. Loss: 0.2590981125831604. Accuracy: 93.94999694824219\n",
      "Iteration: 2000. Loss: 0.1608916074037552. Accuracy: 94.7300033569336\n",
      "Iteration: 2500. Loss: 0.14946995675563812. Accuracy: 95.29000091552734\n",
      "Iteration: 3000. Loss: 0.10177124291658401. Accuracy: 95.62000274658203\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a87636-6222-4fe9-b83e-24a10a23b01b",
   "metadata": {},
   "source": [
    "## 8.2 Convolutional Neural Networks (CNNs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e058bb9-a0ed-4f8c-acff-69053605a86b",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks (CNNs) are a type of artificial neural network designed for image recognition and processing. They are inspired by the structure of the visual cortex, which consists of a hierarchy of filters that process the visual input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b84415e-f4e9-4341-af90-04ec4abe1403",
   "metadata": {},
   "source": [
    "![Image of Runcode](https://codetolight.files.wordpress.com/2017/11/network.png?w=1108)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c5aa4-0237-4e06-9d88-03b5fc9226ee",
   "metadata": {},
   "source": [
    "Here are some key terminologies used in CNNs:\n",
    "\n",
    "1. Convolutional layer: A convolutional layer is a layer of neurons that applies a convolution operation to the input data. The convolution operation involves applying a set of filters (also known as kernels) to the input data to extract features. Each filter is a small matrix that is applied to a region of the input data to generate a feature map.\n",
    "\n",
    "2. Kernel/Filter: A kernel (also known as a filter) is a small matrix that is used to extract features from the input data in a convolutional layer. Each kernel is applied to a region of the input data and generates a feature map by element-wise multiplication and summing the result.\n",
    "\n",
    "3. Stride: The stride is the number of pixels that the kernel moves when it is applied to the input data. A larger stride results in a smaller feature map and reduces the computation time, but may also reduce the accuracy of the model.\n",
    "\n",
    "4. Padding: Padding is the process of adding zeros around the edges of the input data to preserve the spatial dimensions of the output feature map. This allows the kernel to operate on the edges of the input data without reducing the size of the output feature map.\n",
    "\n",
    "5. Pooling layer: A pooling layer is a layer of neurons that applies a pooling operation to the input data. The pooling operation involves down-sampling the input data by taking the maximum (max pooling) or average (average pooling) of a small region of the input data. Pooling layers are used to reduce the computation time and prevent overfitting.\n",
    "\n",
    "6. Receptive field: The receptive field is the region of the input data that is used to compute the output of a neuron in a convolutional layer. The size of the receptive field determines the context that the neuron can see, which affects the ability of the network to recognize patterns in the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d91a009-4cf6-490b-8790-0ca018ae5941",
   "metadata": {},
   "source": [
    "Here is the pytorch code for CNN:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1720d5ad-ca99-4507-8c12-f392ea5cd1f9",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "no_epochs = 15\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./CIFAR_data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./CIFAR_data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 16 * 5 * 5)           \n",
    "        x = F.relu(self.fc1(x))              \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)                       \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "# Training\n",
    "for epoch in range(no_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{no_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# Testing\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3b32223-018f-4ed0-b732-efee32ae5840",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "classes = [\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_dataset[10][0], test_dataset[0][1]\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "    \n",
    "    img = test_dataset[10][0] / 2 + 0.5  \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80186a2-de93-4806-b3d3-875adb0e8871",
   "metadata": {},
   "source": [
    "## 8.3 Recurrent Neural Networks (RNNs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e42f8-cf16-4feb-bc27-ef9196b716f3",
   "metadata": {},
   "source": [
    "Recurrent Neural Networks (RNNs) are a type of artificial neural network designed to process sequential data. They are capable of capturing patterns in the data that are dependent on the sequence of the data, such as time series, natural language, and speech.\n",
    "\n",
    "RNNs use the same weights and biases for all time steps, but they have a hidden state that is updated at each time step and is used to pass information from one time step to the next. This allows RNNs to capture temporal dependencies in the data.\n",
    "\n",
    "There are several techniques that are used to improve the performance of RNNs, including:\n",
    "\n",
    "1. <b>Long Short-Term Memory (LSTM):</b> LSTM is a type of RNN that uses gating mechanisms to allow the network to store and access information over a longer period of time. It is particularly useful for tasks that require the network to remember long-term dependencies.\n",
    "\n",
    "    LSTM networks have a hidden state that is updated at each time step, similar to traditional RNNs. However, they also have three additional gates that control the flow of information into and out of the hidden state: the input gate, the output gate, and the forget gate.\n",
    "\n",
    "    The input gate controls the flow of information into the hidden state and is used to update the hidden state with new information. The output gate controls the flow of information out of the hidden state and is used to generate the output of the LSTM. The forget gate controls the flow of information into the hidden state and is used to forget irrelevant information.\n",
    "\n",
    "2. <b>Gated Recurrent Unit (GRU):</b> GRU is a type of RNN that uses gating mechanisms similar to LSTM, but with a simpler architecture. It has been shown to perform well on a variety of tasks.\n",
    "\n",
    "3. <b>Attention:</b> Attention is a technique that allows the network to selectively focus on certain parts of the input sequence when making predictions. This can be particularly useful for tasks where the relevant information is scattered throughout the input sequence.\n",
    "\n",
    "    <span style=\"color:blue\"><b>Transformers</b></span>: Transformers are a type of neural network architecture that was introduced in the paper \"Attention is All You Need\" (https://arxiv.org/abs/1706.03762). They are primarily used for natural language processing tasks, such as machine translation, language modeling, and text classification.\n",
    "\n",
    "    Transformers are based on the idea of self-attention, which allows the network to attend to different parts of the input sequence at different time steps. This allows the network to capture long-range dependencies in the data without the need for recurrence or convolution.\n",
    "\n",
    "    There are two main components of a transformer: the encoder and the decoder. The encoder processes the input sequence and generates a set of feature vectors (also known as the encoder output). The decoder then uses these feature vectors to generate the output sequence.\n",
    "\n",
    "    The encoder and decoder are composed of a stack of identical layers, each of which consists of a self-attention layer and a feedforward layer. The self-attention layer computes the attention weights for each position in the input sequence and combines the feature vectors of the input sequence using these weights to generate the output feature vectors. The feedforward layer applies a fully-connected neural network to the output feature vectors to generate the final output.\n",
    "    \n",
    "    ![Image of Runcode](https://d2l.ai/_images/transformer.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a185fa7-b13a-4512-a7b2-114308e6f921",
   "metadata": {},
   "source": [
    "RNNs have a wide range of applications, including natural language processing, speech recognition, machine translation, and time series forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47b475c-9ede-4805-9db1-fe9bf1d22897",
   "metadata": {},
   "source": [
    "## 8.4 Generative Adversarial Networks (GANs):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c991e-04fa-4fe7-98de-4d39b48704c8",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) are a type of neural network architecture that is used for unsupervised learning. They consist of two neural networks: a generator network and a discriminator network. The generator network is trained to generate new data samples that are similar to a training dataset, while the discriminator network is trained to distinguish between the generated samples and the real samples from the training dataset.\n",
    "\n",
    "The generator and discriminator networks are trained simultaneously in a zero-sum game, where the generator tries to generate samples that the discriminator cannot distinguish from the real samples, and the discriminator tries to accurately distinguish between the real and generated samples. The objective of the GAN is to find an equilibrium, where the generator generates samples that are indistinguishable from the real samples, and the discriminator is unable to distinguish between the two.\n",
    "\n",
    "\n",
    "![Image of Runcode](https://production-media.paperswithcode.com/methods/gan.jpeg)\n",
    "\n",
    "\n",
    "Here are some key terminologies used in GANs:\n",
    "\n",
    "1. Generator: The generator is a neural network that is trained to generate new data samples that are similar to a training dataset. It takes a random noise input and generates a synthetic data sample.\n",
    "\n",
    "2. Discriminator: The discriminator is a neural network that is trained to distinguish between the generated samples and the real samples from the training dataset. It takes a data sample as input and outputs a probability score indicating the likelihood that the sample is real.\n",
    "\n",
    "3. Noise: Noise is a random input that is fed into the generator network. It is typically drawn from a Gaussian distribution or a uniform distribution.\n",
    "\n",
    "4. Adversarial loss: The adversarial loss is the loss function used to train the generator and discriminator networks. It is defined as the negative log-likelihood of the true labels given the output of the discriminator.\n",
    "\n",
    "The objective of the GAN can be formalized as the following minimax optimization problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf966cd-ae46-40eb-8259-8a730dedc7c2",
   "metadata": {},
   "source": [
    "<b>min_G max_D V(D, G) = E_x[log D(x)] + E_z[log (1 - D(G(z)))]</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6278363-428a-4a18-a046-767e727a945f",
   "metadata": {},
   "source": [
    "Where G is the generator network, D is the discriminator network, x is a real data sample, z is a noise input, and V(D, G) is the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73be8d-1dfa-4ade-905c-8a64ef020411",
   "metadata": {},
   "source": [
    "Here is the pytorch code for Simple GAN network based on fully connected layers and train it on the MNIST dataset:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16a69fb2-55c4-4a9d-b24c-0ed14e6870f2",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, img_dim),\n",
    "            nn.Tanh(),  # normalize inputs to [-1, 1] so make outputs [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        # where the second option of maximizing doesn't suffer from\n",
    "        # saturating gradients\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize=True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize=True)\n",
    "\n",
    "                writer_fake.add_image(\n",
    "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
    "                )\n",
    "                writer_real.add_image(\n",
    "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
    "                )\n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2eeb9-b8c7-4313-8c93-393d1c1cd43f",
   "metadata": {},
   "source": [
    "### Applications of GANs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eb9b7b-be7a-4b17-97a9-324783f79c78",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) have a wide range of applications and have been used in many different domains. Some of the applications and use cases of GANs include:\n",
    "\n",
    "1. Image generation: GANs have been used to generate realistic images from a given set of examples. This has been used for tasks such as generating realistic faces, landscapes, and paintings.\n",
    "\n",
    "2. Image-to-image translation: GANs have been used to translate images from one domain to another. For example, they have been used to transform photographs into paintings, or to transform summer photos into winter photos.\n",
    "\n",
    "3. Text generation: GANs have been used to generate text, including natural language and code. This has been used for tasks such as generating descriptions of images, generating dialogue, and generating programming code.\n",
    "\n",
    "4. Data augmentation: GANs have been used to generate synthetic data samples that can be used to augment the training dataset in supervised learning tasks. This can be particularly useful for tasks where the amount of real data is limited.\n",
    "\n",
    "5. Anomaly detection: GANs have been used to detect anomalous data points in a dataset. The discriminator can be trained to recognize normal data points, and the generator can be used to generate synthetic data points that are similar to the normal data points. Anomalous data points can then be detected by comparing the real data points to the synthetic data points and identifying those that are significantly different.\n",
    "\n",
    "6. Domain adaptation: GANs have been used to adapt a model trained on one dataset to a different dataset. This can be done by training the generator to generate synthetic data points that are similar to the target dataset and using these data points to train the model.\n",
    "\n",
    "7. enerating simulations: GANs have been used to generate synthetic simulations of physical processes, such as fluid dynamics and weather patterns. This can be used to generate training data for predictive models or to perform simulations in a virtual environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
